{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference | Tiny-Mamba Yolo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all dependencies.\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance\n",
    "from ultralytics import YOLO\n",
    "import supervision as sv\n",
    "\n",
    "'''\n",
    "    We evalulate our model on transformed youtube video.\n",
    "    This code contains the followings:\n",
    "        1. Transform downloaded youtube video into train-preprocessed video style.\n",
    "        2. Run the model on the video. \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved as videos/video_transformed.mp4\n"
     ]
    }
   ],
   "source": [
    "def transform(image):\n",
    "    '''\n",
    "        :image: input image frame\n",
    "        \n",
    "        :resize to 1024 by 1024, apply sharpness, contrastness.\n",
    "\n",
    "        return transformed image\n",
    "    '''\n",
    "    img = image.resize((1024, 1024), Image.Resampling.LANCZOS)\n",
    "    sharp = ImageEnhance.Sharpness(img)\n",
    "    img = sharp.enhance(6) \n",
    "    contrast = ImageEnhance.Contrast(img)\n",
    "    img = contrast.enhance(6)  \n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define input_video and transformed_video's path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "''' \n",
    "    We downloaded the video from youtube.\n",
    "    We transform the video and store it at output_video_path\n",
    "    same as we transformed the training dataset\n",
    "'''\n",
    "input_video_path = 'videos/video.mp4' \n",
    "output_video_path = 'videos/video_transformed.mp4'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform each frame and store it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "out = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (1024, 1024))  \n",
    "\n",
    "while True: \n",
    "    '''\n",
    "        gets every frame and apply transform,\n",
    "        and store it in 'video_transformed.mp4'.\n",
    "    '''\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break  \n",
    "    frame_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    transformed_frame = transform(frame_pil)\n",
    "    transformed_frame_cv = cv2.cvtColor(np.array(transformed_frame), cv2.COLOR_RGB2BGR)\n",
    "    out.write(transformed_frame_cv)\n",
    "\n",
    "# release memory\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f\"Video saved as {output_video_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define model Path\n",
    "MODEL_PATH = \"models/last.pt\"\n",
    "model = YOLO(MODEL_PATH)\n",
    "print(model) # same as model.info(). Give summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### A function that gets a frame generator from model\n",
    "\n",
    "def predict(source, model):\n",
    "    ##### Inference on transformed Video or Raw Youtube video. (Raw youtube requires pytube.)\n",
    "    #### Source can be local path, or youtube link.\n",
    "    ### Local path (output_video_path) is expected for model trained on transformed data.\n",
    "    results = model.predict(source, stream=True,imgsz=(1024,1024))\n",
    "    return results  # return frames as a generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display each frame from \"results\" generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = predict(output_video_path, model)\n",
    "for result in results:\n",
    "    boxes = result.boxes  \n",
    "    masks = result.masks \n",
    "    keypoints = result.keypoints \n",
    "    probs = result.probs \n",
    "    obb = result.obb \n",
    "    result.show() # Display each frame"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
